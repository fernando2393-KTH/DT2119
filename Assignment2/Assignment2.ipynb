{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## <p style=\"text-align: center\">DT2119 Lab2: Hidden Markov Models with Gaussian Emissions\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The following packages and files will be used to accomplish the different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from lab2_tools import *\n",
    "from prondict import prondict\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are all the implemented functions which will be used\n",
    "to accomplish the different tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def concatTwoHMMs(hmm1, hmm2):\n",
    "    \"\"\" Concatenates 2 HMM models\n",
    "\n",
    "    Args:\n",
    "       hmm1, hmm2: two dictionaries with the following keys:\n",
    "           - name: phonetic or word symbol corresponding to the model\n",
    "           - startprob: M+1 array with priori probability of state\n",
    "           - transmat: (M+1)x(M+1) transition matrix\n",
    "           - means: MxD array of mean vectors\n",
    "           - covars: MxD array of variances\n",
    "\n",
    "    D is the dimension of the feature vectors\n",
    "    M is the number of emitting states in each HMM model (could be different for each)\n",
    "\n",
    "    Output\n",
    "       dictionary with the same keys as the input but concatenated models:\n",
    "          startprob: K+1 array with priori probability of state\n",
    "          transmat: (K+1)x(K+1) transition matrix\n",
    "             means: KxD array of mean vectors\n",
    "            covars: KxD array of variances\n",
    "\n",
    "    K is the sum of the number of emitting states from the input models\n",
    "   \n",
    "    Example:\n",
    "       twoHMMs = concatHMMs(phoneHMMs['sil'], phoneHMMs['ow'])\n",
    "\n",
    "    See also: the concatenating_hmms.pdf document in the lab package\n",
    "    \"\"\"\n",
    "\n",
    "    concat_hmm = {}\n",
    "    startprob = np.zeros(hmm1['startprob'].shape[0] + hmm2['startprob'].shape[0] - 1)\n",
    "    for i in range(startprob.shape[0]):\n",
    "        if i < hmm1['startprob'].shape[0] - 1:\n",
    "            startprob[i] = hmm1['startprob'][i]\n",
    "        else:\n",
    "            startprob[i] = hmm1['startprob'][-1] * hmm2['startprob'][i - (hmm1['startprob'].shape[0] - 1)]\n",
    "\n",
    "    transmat = np.zeros((startprob.shape[0], startprob.shape[0]))\n",
    "    for i in range(transmat.shape[0] - 1):\n",
    "        for j in range(transmat.shape[1]):\n",
    "            if i < hmm1['transmat'].shape[0] - 1 and j < hmm1['transmat'].shape[1] - 1:  # Copy of hmm1 values\n",
    "                transmat[i, j] = hmm1['transmat'][i, j]\n",
    "            elif i < hmm1['transmat'].shape[0] - 1:  # Product of the last value of hmm1 and values of hmm2\n",
    "                transmat[i, j] = hmm1['transmat'][i, -1] * hmm2['startprob'][j - (hmm1['transmat'].shape[1] - 1)]\n",
    "            elif j >= hmm1['transmat'].shape[1] - 1:  # Copy of hmm2 values\n",
    "                transmat[i, j] = hmm2['transmat'][i - (hmm1['transmat'].shape[0] - 1),\n",
    "                                                  j - (hmm1['transmat'].shape[1] - 1)]\n",
    "    transmat[-1, -1] = 1  # Assign value 1 to the last transition\n",
    "\n",
    "    means = np.vstack((hmm1['means'], hmm2['means']))\n",
    "    covars = np.vstack((hmm1['covars'], hmm2['covars']))\n",
    "\n",
    "    concat_hmm['startprob'] = startprob\n",
    "    concat_hmm['transmat'] = transmat\n",
    "    concat_hmm['means'] = means\n",
    "    concat_hmm['covars'] = covars\n",
    "\n",
    "    return concat_hmm\n",
    "\n",
    "\n",
    "# this is already implemented, but based on concat2HMMs() above\n",
    "def concatHMMs(hmmmodels, namelist):\n",
    "    \"\"\" Concatenates HMM models in a left to right manner\n",
    "\n",
    "    Args:\n",
    "       hmmmodels: dictionary of models indexed by model name. \n",
    "       hmmmodels[name] is a dictionaries with the following keys:\n",
    "           - name: phonetic or word symbol corresponding to the model\n",
    "           - startprob: M+1 array with priori probability of state\n",
    "           - transmat: (M+1)x(M+1) transition matrix\n",
    "           - means: MxD array of mean vectors\n",
    "           - covars: MxD array of variances\n",
    "       namelist: list of model names that we want to concatenate\n",
    "\n",
    "    D is the dimension of the feature vectors\n",
    "    M is the number of emitting states in each HMM model (could be\n",
    "      different in each model)\n",
    "\n",
    "    Output\n",
    "       combinedhmm: dictionary with the same keys as the input but\n",
    "                    combined models:\n",
    "         startprob: K+1 array with priori probability of state\n",
    "          transmat: (K+1)x(K+1) transition matrix\n",
    "             means: KxD array of mean vectors\n",
    "            covars: KxD array of variances\n",
    "\n",
    "    K is the sum of the number of emitting states from the input models\n",
    "\n",
    "    Example:\n",
    "       wordHMMs['o'] = concatHMMs(phoneHMMs, ['sil', 'ow', 'sil'])\n",
    "    \"\"\"\n",
    "    concat = hmmmodels[namelist[0]]\n",
    "    for idx in range(1, len(namelist)):\n",
    "        concat = concatTwoHMMs(concat, hmmmodels[namelist[idx]])\n",
    "\n",
    "    return concat\n",
    "\n",
    "\n",
    "def gmmloglik(log_emlik, weights):\n",
    "    \"\"\"Log Likelihood for a GMM model based on Multivariate Normal Distribution.\n",
    "\n",
    "    Args:\n",
    "        log_emlik: array like, shape (N, K).\n",
    "            contains the log likelihoods for each of N observations and\n",
    "            each of K distributions\n",
    "        weights:   weight vector for the K components in the mixture\n",
    "\n",
    "    Output:\n",
    "        gmmloglik: scalar, log likelihood of data given the GMM model.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def forward(log_emlik, log_startprob, log_transmat):\n",
    "    \"\"\"Forward (alpha) probabilities in log domain.\n",
    "\n",
    "    Args:\n",
    "        log_emlik: NxM array of emission log likelihoods, N frames, M states\n",
    "        log_startprob: log probability to start in state i\n",
    "        log_transmat: log transition probability from state i to j\n",
    "\n",
    "    Output:\n",
    "        forward_prob: NxM array of forward log probabilities for each of the M states in the model\n",
    "    \"\"\"\n",
    "\n",
    "    forward_prob = np.zeros(log_emlik.shape)\n",
    "    forward_prob[0, :] = log_startprob[:-1] + log_emlik[0]  # Use all states but the last one --> ending state\n",
    "\n",
    "    for n in range(1, forward_prob.shape[0]):\n",
    "        for j in range(forward_prob.shape[1]):\n",
    "            forward_prob[n, j] = logsumexp(forward_prob[n - 1, :] + log_transmat[:-1, j]) + log_emlik[n, j]\n",
    "\n",
    "    return forward_prob\n",
    "\n",
    "\n",
    "def backward(log_emlik, log_transmat):\n",
    "    \"\"\"Backward (beta) probabilities in log domain.\n",
    "\n",
    "    Args:\n",
    "        log_emlik: NxM array of emission log likelihoods, N frames, M states\n",
    "        log_transmat: transition log probability from state i to j\n",
    "\n",
    "    Output:\n",
    "        backward_prob: NxM array of backward log probabilities for each of the M states in the model\n",
    "    \"\"\"\n",
    "\n",
    "    backward_prob = np.zeros(log_emlik.shape)\n",
    "\n",
    "    for n in reversed(range(backward_prob.shape[0] - 1)):\n",
    "        for i in range(backward_prob.shape[1]):\n",
    "            backward_prob[n, i] = logsumexp(log_transmat[i, :-1] + log_emlik[n + 1, :] + backward_prob[n + 1, :])\n",
    "\n",
    "    return backward_prob\n",
    "\n",
    "\n",
    "def viterbi(log_emlik, log_startprob, log_transmat, force_final_state=True):\n",
    "    \"\"\"Viterbi path.\n",
    "\n",
    "    Args:\n",
    "        log_emlik: NxM array of emission log likelihoods, N frames, M states\n",
    "        log_startprob: log probability to start in state i\n",
    "        log_transmat: transition log probability from state i to j\n",
    "        force_final_state: if True, start backtracking from the final state in\n",
    "                  the model, instead of the best state at the last time step\n",
    "\n",
    "    Output:\n",
    "        viterbi_loglik: log likelihood of the best path\n",
    "        viterbi_path: best path\n",
    "    \"\"\"\n",
    "\n",
    "    viterbi_loglik = np.zeros(log_emlik.shape)\n",
    "    viterbi_b_mat = np.zeros(log_emlik.shape, dtype=int)\n",
    "\n",
    "    viterbi_loglik[0] = log_startprob[:-1] + log_emlik[0]\n",
    "    for n in range(1, viterbi_loglik.shape[0]):\n",
    "        for j in range(viterbi_loglik.shape[1]):\n",
    "            viterbi_loglik[n, j] = np.max(viterbi_loglik[n - 1, :] + log_transmat[:-1, j]) + log_emlik[n, j]\n",
    "            viterbi_b_mat[n, j] = np.argmax(viterbi_loglik[n - 1, :] + log_transmat[:-1, j])\n",
    "\n",
    "    viterbi_path = [np.argmax(viterbi_b_mat[-1])]\n",
    "    for n in reversed(range(viterbi_b_mat.shape[0] - 1)):\n",
    "        viterbi_path.append(viterbi_b_mat[n, viterbi_path[-1]])\n",
    "    viterbi_path.reverse()\n",
    "\n",
    "    return np.max(viterbi_loglik[-1]), np.array(viterbi_path)\n",
    "\n",
    "\n",
    "def statePosteriors(log_alpha, log_beta):\n",
    "    \"\"\"State posterior (gamma) probabilities in log domain.\n",
    "\n",
    "    Args:\n",
    "        log_alpha: NxM array of log forward (alpha) probabilities\n",
    "        log_beta: NxM array of log backward (beta) probabilities\n",
    "    where N is the number of frames, and M the number of states\n",
    "\n",
    "    Output:\n",
    "        log_gamma: NxM array of gamma probabilities for each of the M states in the model\n",
    "    \"\"\"\n",
    "\n",
    "    log_gamma = log_alpha + log_beta - logsumexp(log_alpha[log_alpha.shape[0] - 1])\n",
    "\n",
    "    return log_gamma\n",
    "\n",
    "\n",
    "def updateMeanAndVar(x, log_gamma, variance_floor=5.0):\n",
    "    \"\"\" Update Gaussian parameters with diagonal covariance\n",
    "\n",
    "    Args:\n",
    "         x: NxD array of feature vectors\n",
    "         log_gamma: NxM state posterior probabilities in log domain\n",
    "         variance_floor: minimum allowed variance scalar\n",
    "    were N is the lenght of the observation sequence, D is the\n",
    "    dimensionality of the feature vectors and M is the number of\n",
    "    states in the model\n",
    "\n",
    "    Outputs:\n",
    "         means: MxD mean vectors for each state\n",
    "         covars: MxD covariance (variance) vectors for each state\n",
    "    \"\"\"\n",
    "\n",
    "    gamma = np.exp(log_gamma)\n",
    "\n",
    "    means = np.zeros((log_gamma.shape[1], x.shape[1]))\n",
    "    covars = np.zeros(means.shape)\n",
    "    for k in range(means.shape[0]):\n",
    "        means[k] = np.sum(gamma[:, k][:, np.newaxis] * x, axis=0) / np.sum(gamma[:, k])\n",
    "        covars[k] = np.sum(gamma[:, k][:, np.newaxis] * np.power(x - means[k], 2), axis=0) / np.sum(gamma[:, k])\n",
    "    covars[covars < variance_floor] = variance_floor  # Transform values below threshold\n",
    "\n",
    "    return means, covars\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Here starts the program main code:\n",
    "Load of the data samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore')  # Suppress divide by zero warning\n",
    "example = np.load('lab2_example.npz', allow_pickle=True)['example'].item()\n",
    "phone_hhms = np.load('lab2_models_all.npz', allow_pickle=True)['phoneHMMs'].item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " Concatenate all digit hmms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "word_hmms = {}\n",
    "for digit in prondict.keys():\n",
    "    word_hmms[digit] = concatHMMs(phone_hhms, ['sil'] + prondict[digit] + ['sil'])\n",
    "\n",
    "data = np.load('lab2_data.npz', allow_pickle=True)['data']\n",
    "best_model = {}\n",
    "acc_count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Forward algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the predictions has been: 97.73%\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for idx, dt in enumerate(data):  # Iterate over data samples\n",
    "    maxloglik = None\n",
    "    for digit in word_hmms.keys():  # Iterate over hmms\n",
    "        obsloglik = log_multivariate_normal_density_diag(dt['lmfcc'], word_hmms[digit]['means'],\n",
    "                                                         word_hmms[digit]['covars'])\n",
    "        logalpha = forward(obsloglik, np.log(word_hmms[digit]['startprob']),\n",
    "                           np.log(word_hmms[digit]['transmat']))\n",
    "        loglik = logsumexp(logalpha[-1])\n",
    "        if maxloglik is None or maxloglik < loglik:  # If better likelihood found\n",
    "            best_model[idx] = digit  # Set most probable model\n",
    "            maxloglik = loglik  # Update max log likelihood\n",
    "    if dt['digit'] == best_model[idx]:\n",
    "        acc_count += 1\n",
    "        \n",
    "print(\"The accuracy of the predictions has been: \" + str(np.round(acc_count / len(data) * 100, 2)) + \"%\")\n",
    "np.seterr(divide='ignore')  # Suppress divide by zero warning\n",
    "logalpha = forward(example['obsloglik'], np.log(word_hmms['o']['startprob']), np.log(word_hmms['o']['transmat']))\n",
    "vloglik, vpath = viterbi(example['obsloglik'], np.log(word_hmms['o']['startprob']),\n",
    "                         np.log(word_hmms['o']['transmat']))\n",
    "plt.pcolormesh(logalpha.T)\n",
    "plt.plot(vpath.T, color=\"red\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Viterbi algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy of the predictions has been: 100.0%\n"
     ]
    }
   ],
   "source": [
    "best_model = {}\n",
    "acc_count = 0\n",
    "for idx, dt in enumerate(data):  # Iterate over data samples\n",
    "    maxloglik = None\n",
    "    for digit in word_hmms.keys():  # Iterate over hmms\n",
    "        obsloglik = log_multivariate_normal_density_diag(dt['lmfcc'], word_hmms[digit]['means'],\n",
    "                                                         word_hmms[digit]['covars'])\n",
    "        vloglik, vpath = viterbi(obsloglik, np.log(word_hmms[digit]['startprob']),\n",
    "                                 np.log(word_hmms[digit]['transmat']))\n",
    "        if maxloglik is None or maxloglik < vloglik:  # If better likelihood found\n",
    "            best_model[idx] = digit  # Set most probable model\n",
    "            maxloglik = vloglik  # Update max log likelihood\n",
    "    if dt['digit'] == best_model[idx]:\n",
    "        acc_count += 1\n",
    "    # print(\"The best model for utterance \" + str(idx) + \" was hmm: \" + str(best_model[idx]))\n",
    "    # print(\"The real digit of utterance \" + str(idx) + \" was digit: \" + str(dt['digit']) + \"\\n\")\n",
    "print(\"The accuracy of the predictions has been: \" + str(np.round(acc_count / len(data) * 100, 2)) + \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Baum-Welch algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying model o\n",
      "Log-likelihood: -6318.836143374648. Number of iterations until convergence: 5\n",
      "Trying model z\n",
      "Log-likelihood: -5833.270624049372. Number of iterations until convergence: 5\n",
      "Trying model 1\n",
      "Log-likelihood: -5903.338261970013. Number of iterations until convergence: 10\n",
      "Trying model 2\n",
      "Log-likelihood: -6190.713363055121. Number of iterations until convergence: 5\n",
      "Trying model 3\n",
      "Log-likelihood: -6012.101687040111. Number of iterations until convergence: 4\n",
      "Trying model 4\n",
      "Log-likelihood: -5994.0508169008335. Number of iterations until convergence: 5\n",
      "Trying model 5\n",
      "Log-likelihood: -5963.9224257326605. Number of iterations until convergence: 5\n",
      "Trying model 6\n",
      "Log-likelihood: -5884.528943767898. Number of iterations until convergence: 5\n",
      "Trying model 7\n",
      "Log-likelihood: -5682.161964496247. Number of iterations until convergence: 5\n",
      "Trying model 8\n",
      "Log-likelihood: -6185.971631860034. Number of iterations until convergence: 4\n",
      "Trying model 9\n",
      "Log-likelihood: -5953.968391594922. Number of iterations until convergence: 5\n",
      "The best log-likelihood is: -5682.161964496247. It corresponds to the model 7\n"
     ]
    }
   ],
   "source": [
    "best_loglik = None\n",
    "best_model = None\n",
    "for digit in word_hmms.keys():  # Iterate over hmms\n",
    "    print(\"Trying model \" + str(digit))\n",
    "    means = word_hmms[digit]['means']\n",
    "    covars = word_hmms[digit]['covars']\n",
    "    obsloglik = log_multivariate_normal_density_diag(data[10]['lmfcc'], means, covars)\n",
    "    vloglik = 0\n",
    "    newloglik = viterbi(obsloglik, np.log(word_hmms[digit]['startprob']),\n",
    "                        np.log(word_hmms[digit]['transmat']))[0]\n",
    "    it = 0\n",
    "    while it < 20 and abs(newloglik - vloglik) > 1.0:\n",
    "        vloglik = newloglik  # Update value of log-likelihood\n",
    "        forward_prob = forward(obsloglik, np.log(word_hmms[digit]['startprob']),\n",
    "                               np.log(word_hmms[digit]['transmat']))\n",
    "        backward_prob = backward(obsloglik, np.log(word_hmms[digit]['transmat']))\n",
    "        log_gamma = statePosteriors(forward_prob, backward_prob)\n",
    "        means, covars = updateMeanAndVar(data[10]['lmfcc'], log_gamma)\n",
    "        obsloglik = log_multivariate_normal_density_diag(data[10]['lmfcc'], means, covars)\n",
    "        newloglik = viterbi(obsloglik, np.log(word_hmms[digit]['startprob']),\n",
    "                            np.log(word_hmms[digit]['transmat']))[0]\n",
    "        it += 1  # Update number of iterations\n",
    "    print(\"Log-likelihood: \" + str(newloglik) + \". Number of iterations until convergence: \" + str(it))\n",
    "    if best_loglik is None or newloglik > best_loglik:\n",
    "        best_loglik = newloglik\n",
    "        best_model = digit\n",
    "print(\"The best log-likelihood is: \" + str(best_loglik) + \". It corresponds to the model \" + str(best_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
